{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Dateset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(ConvolutionalVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten() # Flatten the output of the convolutional encoder\n",
    "        self.fc_mu = nn.Linear(32 * 7 * 7, latent_dim)  # Adjust the input size based on your image size and encoder layers\n",
    "        self.fc_logvar = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "        self.unflatten = nn.Unflatten(1, (32, 7, 7)) # Unflatten before the decoder\n",
    "        self.linear_decoder = nn.Linear(latent_dim, 32 * 7 * 7) \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        hidden = self.flatten(hidden) # Flatten for the fully connected layers\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        z = self.linear_decoder(z)\n",
    "        z = self.unflatten(z) # Unflatten before the convolutional decoder\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "\n",
    "def loss_function(reconstructed_x, x, mu, logvar):\n",
    "    reconstruction_loss = nn.MSELoss()(reconstructed_x, x)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + kl_divergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  10 Loss:  2.306786298751831\n",
      "iteration:  20 Loss:  1.557590126991272\n",
      "iteration:  30 Loss:  1.3530007600784302\n",
      "iteration:  40 Loss:  1.2610061168670654\n",
      "iteration:  50 Loss:  1.1733235120773315\n",
      "iteration:  60 Loss:  1.123488426208496\n",
      "iteration:  70 Loss:  1.0798426866531372\n",
      "iteration:  80 Loss:  1.0956699848175049\n",
      "iteration:  90 Loss:  1.0120761394500732\n",
      "iteration:  100 Loss:  0.9585146903991699\n",
      "iteration:  110 Loss:  0.9695087671279907\n",
      "iteration:  120 Loss:  0.935856819152832\n",
      "iteration:  130 Loss:  0.9504994750022888\n",
      "iteration:  140 Loss:  0.8869339227676392\n",
      "iteration:  150 Loss:  0.8392549753189087\n",
      "iteration:  160 Loss:  0.8579514026641846\n",
      "iteration:  170 Loss:  0.8511385321617126\n",
      "iteration:  180 Loss:  0.8703644871711731\n",
      "iteration:  190 Loss:  0.8119795322418213\n",
      "iteration:  200 Loss:  0.8327888250350952\n",
      "iteration:  210 Loss:  0.8342608213424683\n",
      "iteration:  220 Loss:  0.8262317776679993\n",
      "iteration:  230 Loss:  0.8024864792823792\n",
      "iteration:  240 Loss:  0.8170163631439209\n",
      "iteration:  250 Loss:  0.8346086740493774\n",
      "iteration:  260 Loss:  0.8077362775802612\n",
      "iteration:  270 Loss:  0.7908524870872498\n",
      "iteration:  280 Loss:  0.7993702292442322\n",
      "iteration:  290 Loss:  0.8533453941345215\n",
      "iteration:  300 Loss:  0.849056601524353\n",
      "iteration:  310 Loss:  0.7750768065452576\n",
      "iteration:  320 Loss:  0.8391947746276855\n",
      "iteration:  330 Loss:  0.8305529952049255\n",
      "iteration:  340 Loss:  0.8375181555747986\n",
      "iteration:  350 Loss:  0.7956610321998596\n",
      "iteration:  360 Loss:  0.8215344548225403\n",
      "iteration:  370 Loss:  0.8354426622390747\n",
      "iteration:  380 Loss:  0.8097729682922363\n",
      "iteration:  390 Loss:  0.7904932498931885\n",
      "iteration:  400 Loss:  0.8331198692321777\n",
      "iteration:  410 Loss:  0.8682436943054199\n",
      "iteration:  420 Loss:  0.8291343450546265\n",
      "iteration:  430 Loss:  0.8242999911308289\n",
      "iteration:  440 Loss:  0.8144463300704956\n",
      "iteration:  450 Loss:  0.8312652707099915\n",
      "iteration:  460 Loss:  0.7877765893936157\n",
      "iteration:  470 Loss:  0.844517707824707\n",
      "iteration:  480 Loss:  0.822728157043457\n",
      "iteration:  490 Loss:  0.8118000626564026\n",
      "iteration:  500 Loss:  0.7988573908805847\n",
      "iteration:  510 Loss:  0.778489351272583\n",
      "iteration:  520 Loss:  0.7928030490875244\n",
      "iteration:  530 Loss:  0.8195687532424927\n",
      "iteration:  540 Loss:  0.8415353298187256\n",
      "iteration:  550 Loss:  0.8182677030563354\n",
      "iteration:  560 Loss:  0.8189808130264282\n",
      "iteration:  570 Loss:  0.8237028121948242\n",
      "iteration:  580 Loss:  0.7994946241378784\n",
      "iteration:  590 Loss:  0.8312622904777527\n",
      "iteration:  600 Loss:  0.7911876440048218\n",
      "iteration:  610 Loss:  0.8192212581634521\n",
      "iteration:  620 Loss:  0.8153375387191772\n",
      "iteration:  630 Loss:  0.8213309049606323\n",
      "iteration:  640 Loss:  0.7974888682365417\n",
      "iteration:  650 Loss:  0.8567180633544922\n",
      "iteration:  660 Loss:  0.8492867946624756\n",
      "iteration:  670 Loss:  0.809402585029602\n",
      "iteration:  680 Loss:  0.8235794305801392\n",
      "iteration:  690 Loss:  0.8193680644035339\n",
      "iteration:  700 Loss:  0.8213403224945068\n",
      "iteration:  710 Loss:  0.8520117998123169\n",
      "iteration:  720 Loss:  0.8311248421669006\n",
      "iteration:  730 Loss:  0.8002581596374512\n",
      "iteration:  740 Loss:  0.7580683827400208\n",
      "iteration:  750 Loss:  0.825897753238678\n"
     ]
    }
   ],
   "source": [
    "vae_model = ConvolutionalVAE(latent_dim = 64)\n",
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr=0.001)\n",
    "\n",
    "epoch = 1\n",
    "for i in range(epoch):\n",
    "    ite = 0\n",
    "    for x,y in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, mu, logvar  = vae_model(x)\n",
    "        loss = loss_function(outputs, x, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ite += 1\n",
    "        \n",
    "        if ite % 10 == 0:\n",
    "            print(\"iteration: \", ite , \"Loss: \", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 100, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         ...,\n",
       "         [ 3.5342e-04,  5.5507e-05,  4.0163e-04,  ..., -8.1336e-04,\n",
       "          -5.3792e-04, -2.8998e-04],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05]],\n",
       "\n",
       "        [[-9.0247e-06,  1.6637e-05,  1.1014e-05,  ...,  1.4341e-05,\n",
       "          -1.2198e-05,  1.6358e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         ...,\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05]],\n",
       "\n",
       "        [[-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         ...,\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         ...,\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05]],\n",
       "\n",
       "        [[-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         ...,\n",
       "         [-2.4306e-05,  1.3609e-04,  2.8757e-04,  ...,  1.4353e-04,\n",
       "           3.6406e-04,  1.5447e-04],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05]],\n",
       "\n",
       "        [[-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         ...,\n",
       "         [-5.5582e-04, -1.2662e-03,  9.0600e-04,  ...,  9.0835e-04,\n",
       "           1.2863e-03,  1.2382e-03],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05],\n",
       "         [-1.4248e-05,  2.8832e-05,  1.4438e-05,  ...,  1.3050e-06,\n",
       "          -9.5947e-06,  2.5988e-05]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model.eval\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu_list = []\n",
    "    logvar_list = []\n",
    "    for x,y in test_loader:\n",
    "        \n",
    "        _, mu, logvar = vae_model(x)\n",
    "        mu_list.append(mu)\n",
    "        logvar_list.append(logvar)\n",
    "\n",
    "logvar_list = torch.stack(logvar_list)     \n",
    "mu_list = torch.stack(mu_list)\n",
    "print(mu_list.shape)\n",
    "mu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
