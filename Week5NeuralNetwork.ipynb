{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Week 5 Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([318, 7]), torch.Size([318, 1])\n",
      "Test shape: torch.Size([80, 7]), torch.Size([80, 1])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('input/auto-mpg[1].csv')\n",
    "data.replace('?',np.nan, inplace= True)\n",
    "data['horsepower'] = pd.to_numeric(data['horsepower'], errors='coerce')\n",
    "data['horsepower'].fillna(data['horsepower'].median(), inplace = True)\n",
    "data = data.drop(['car name'], axis=1)\n",
    "target = data['mpg']\n",
    "features = data.drop(['mpg'], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(features.values, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(target.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "print(f'Train shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test shape: {X_test.shape}, {y_test.shape}')\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=64, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]\n",
      "Epoch [2/300]\n",
      "Epoch [3/300]\n",
      "Epoch [4/300]\n",
      "Epoch [5/300]\n",
      "Epoch [6/300]\n",
      "Epoch [7/300]\n",
      "Epoch [8/300]\n",
      "Epoch [9/300]\n",
      "Epoch [10/300]\n",
      "Epoch [11/300]\n",
      "Epoch [12/300]\n",
      "Epoch [13/300]\n",
      "Epoch [14/300]\n",
      "Epoch [15/300]\n",
      "Epoch [16/300]\n",
      "Epoch [17/300]\n",
      "Epoch [18/300]\n",
      "Epoch [19/300]\n",
      "Epoch [20/300]\n",
      "Epoch [21/300]\n",
      "Epoch [22/300]\n",
      "Epoch [23/300]\n",
      "Epoch [24/300]\n",
      "Epoch [25/300]\n",
      "Epoch [26/300]\n",
      "Epoch [27/300]\n",
      "Epoch [28/300]\n",
      "Epoch [29/300]\n",
      "Epoch [30/300]\n",
      "Epoch [31/300]\n",
      "Epoch [32/300]\n",
      "Epoch [33/300]\n",
      "Epoch [34/300]\n",
      "Epoch [35/300]\n",
      "Epoch [36/300]\n",
      "Epoch [37/300]\n",
      "Epoch [38/300]\n",
      "Epoch [39/300]\n",
      "Epoch [40/300]\n",
      "Epoch [41/300]\n",
      "Epoch [42/300]\n",
      "Epoch [43/300]\n",
      "Epoch [44/300]\n",
      "Epoch [45/300]\n",
      "Epoch [46/300]\n",
      "Epoch [47/300]\n",
      "Epoch [48/300]\n",
      "Epoch [49/300]\n",
      "Epoch [50/300]\n",
      "Epoch [51/300]\n",
      "Epoch [52/300]\n",
      "Epoch [53/300]\n",
      "Epoch [54/300]\n",
      "Epoch [55/300]\n",
      "Epoch [56/300]\n",
      "Epoch [57/300]\n",
      "Epoch [58/300]\n",
      "Epoch [59/300]\n",
      "Epoch [60/300]\n",
      "Epoch [61/300]\n",
      "Epoch [62/300]\n",
      "Epoch [63/300]\n",
      "Epoch [64/300]\n",
      "Epoch [65/300]\n",
      "Epoch [66/300]\n",
      "Epoch [67/300]\n",
      "Epoch [68/300]\n",
      "Epoch [69/300]\n",
      "Epoch [70/300]\n",
      "Epoch [71/300]\n",
      "Epoch [72/300]\n",
      "Epoch [73/300]\n",
      "Epoch [74/300]\n",
      "Epoch [75/300]\n",
      "Epoch [76/300]\n",
      "Epoch [77/300]\n",
      "Epoch [78/300]\n",
      "Epoch [79/300]\n",
      "Epoch [80/300]\n",
      "Epoch [81/300]\n",
      "Epoch [82/300]\n",
      "Epoch [83/300]\n",
      "Epoch [84/300]\n",
      "Epoch [85/300]\n",
      "Epoch [86/300]\n",
      "Epoch [87/300]\n",
      "Epoch [88/300]\n",
      "Epoch [89/300]\n",
      "Epoch [90/300]\n",
      "Epoch [91/300]\n",
      "Epoch [92/300]\n",
      "Epoch [93/300]\n",
      "Epoch [94/300]\n",
      "Epoch [95/300]\n",
      "Epoch [96/300]\n",
      "Epoch [97/300]\n",
      "Epoch [98/300]\n",
      "Epoch [99/300]\n",
      "Epoch [100/300]\n",
      "Epoch [101/300]\n",
      "Epoch [102/300]\n",
      "Epoch [103/300]\n",
      "Epoch [104/300]\n",
      "Epoch [105/300]\n",
      "Epoch [106/300]\n",
      "Epoch [107/300]\n",
      "Epoch [108/300]\n",
      "Epoch [109/300]\n",
      "Epoch [110/300]\n",
      "Epoch [111/300]\n",
      "Epoch [112/300]\n",
      "Epoch [113/300]\n",
      "Epoch [114/300]\n",
      "Epoch [115/300]\n",
      "Epoch [116/300]\n",
      "Epoch [117/300]\n",
      "Epoch [118/300]\n",
      "Epoch [119/300]\n",
      "Epoch [120/300]\n",
      "Epoch [121/300]\n",
      "Epoch [122/300]\n",
      "Epoch [123/300]\n",
      "Epoch [124/300]\n",
      "Epoch [125/300]\n",
      "Epoch [126/300]\n",
      "Epoch [127/300]\n",
      "Epoch [128/300]\n",
      "Epoch [129/300]\n",
      "Epoch [130/300]\n",
      "Epoch [131/300]\n",
      "Epoch [132/300]\n",
      "Epoch [133/300]\n",
      "Epoch [134/300]\n",
      "Epoch [135/300]\n",
      "Epoch [136/300]\n",
      "Epoch [137/300]\n",
      "Epoch [138/300]\n",
      "Epoch [139/300]\n",
      "Epoch [140/300]\n",
      "Epoch [141/300]\n",
      "Epoch [142/300]\n",
      "Epoch [143/300]\n",
      "Epoch [144/300]\n",
      "Epoch [145/300]\n",
      "Epoch [146/300]\n",
      "Epoch [147/300]\n",
      "Epoch [148/300]\n",
      "Epoch [149/300]\n",
      "Epoch [150/300]\n",
      "Epoch [151/300]\n",
      "Epoch [152/300]\n",
      "Epoch [153/300]\n",
      "Epoch [154/300]\n",
      "Epoch [155/300]\n",
      "Epoch [156/300]\n",
      "Epoch [157/300]\n",
      "Epoch [158/300]\n",
      "Epoch [159/300]\n",
      "Epoch [160/300]\n",
      "Epoch [161/300]\n",
      "Epoch [162/300]\n",
      "Epoch [163/300]\n",
      "Epoch [164/300]\n",
      "Epoch [165/300]\n",
      "Epoch [166/300]\n",
      "Epoch [167/300]\n",
      "Epoch [168/300]\n",
      "Epoch [169/300]\n",
      "Epoch [170/300]\n",
      "Epoch [171/300]\n",
      "Epoch [172/300]\n",
      "Epoch [173/300]\n",
      "Epoch [174/300]\n",
      "Epoch [175/300]\n",
      "Epoch [176/300]\n",
      "Epoch [177/300]\n",
      "Epoch [178/300]\n",
      "Epoch [179/300]\n",
      "Epoch [180/300]\n",
      "Epoch [181/300]\n",
      "Epoch [182/300]\n",
      "Epoch [183/300]\n",
      "Epoch [184/300]\n",
      "Epoch [185/300]\n",
      "Epoch [186/300]\n",
      "Epoch [187/300]\n",
      "Epoch [188/300]\n",
      "Epoch [189/300]\n",
      "Epoch [190/300]\n",
      "Epoch [191/300]\n",
      "Epoch [192/300]\n",
      "Epoch [193/300]\n",
      "Epoch [194/300]\n",
      "Epoch [195/300]\n",
      "Epoch [196/300]\n",
      "Epoch [197/300]\n",
      "Epoch [198/300]\n",
      "Epoch [199/300]\n",
      "Epoch [200/300]\n",
      "Epoch [201/300]\n",
      "Epoch [202/300]\n",
      "Epoch [203/300]\n",
      "Epoch [204/300]\n",
      "Epoch [205/300]\n",
      "Epoch [206/300]\n",
      "Epoch [207/300]\n",
      "Epoch [208/300]\n",
      "Epoch [209/300]\n",
      "Epoch [210/300]\n",
      "Epoch [211/300]\n",
      "Epoch [212/300]\n",
      "Epoch [213/300]\n",
      "Epoch [214/300]\n",
      "Epoch [215/300]\n",
      "Epoch [216/300]\n",
      "Epoch [217/300]\n",
      "Epoch [218/300]\n",
      "Epoch [219/300]\n",
      "Epoch [220/300]\n",
      "Epoch [221/300]\n",
      "Epoch [222/300]\n",
      "Epoch [223/300]\n",
      "Epoch [224/300]\n",
      "Epoch [225/300]\n",
      "Epoch [226/300]\n",
      "Epoch [227/300]\n",
      "Epoch [228/300]\n",
      "Epoch [229/300]\n",
      "Epoch [230/300]\n",
      "Epoch [231/300]\n",
      "Epoch [232/300]\n",
      "Epoch [233/300]\n",
      "Epoch [234/300]\n",
      "Epoch [235/300]\n",
      "Epoch [236/300]\n",
      "Epoch [237/300]\n",
      "Epoch [238/300]\n",
      "Epoch [239/300]\n",
      "Epoch [240/300]\n",
      "Epoch [241/300]\n",
      "Epoch [242/300]\n",
      "Epoch [243/300]\n",
      "Epoch [244/300]\n",
      "Epoch [245/300]\n",
      "Epoch [246/300]\n",
      "Epoch [247/300]\n",
      "Epoch [248/300]\n",
      "Epoch [249/300]\n",
      "Epoch [250/300]\n",
      "Epoch [251/300]\n",
      "Epoch [252/300]\n",
      "Epoch [253/300]\n",
      "Epoch [254/300]\n",
      "Epoch [255/300]\n",
      "Epoch [256/300]\n",
      "Epoch [257/300]\n",
      "Epoch [258/300]\n",
      "Epoch [259/300]\n",
      "Epoch [260/300]\n",
      "Epoch [261/300]\n",
      "Epoch [262/300]\n",
      "Epoch [263/300]\n",
      "Epoch [264/300]\n",
      "Epoch [265/300]\n",
      "Epoch [266/300]\n",
      "Epoch [267/300]\n",
      "Epoch [268/300]\n",
      "Epoch [269/300]\n",
      "Epoch [270/300]\n",
      "Epoch [271/300]\n",
      "Epoch [272/300]\n",
      "Epoch [273/300]\n",
      "Epoch [274/300]\n",
      "Epoch [275/300]\n",
      "Epoch [276/300]\n",
      "Epoch [277/300]\n",
      "Epoch [278/300]\n",
      "Epoch [279/300]\n",
      "Epoch [280/300]\n",
      "Epoch [281/300]\n",
      "Epoch [282/300]\n",
      "Epoch [283/300]\n",
      "Epoch [284/300]\n",
      "Epoch [285/300]\n",
      "Epoch [286/300]\n",
      "Epoch [287/300]\n",
      "Epoch [288/300]\n",
      "Epoch [289/300]\n",
      "Epoch [290/300]\n",
      "Epoch [291/300]\n",
      "Epoch [292/300]\n",
      "Epoch [293/300]\n",
      "Epoch [294/300]\n",
      "Epoch [295/300]\n",
      "Epoch [296/300]\n",
      "Epoch [297/300]\n",
      "Epoch [298/300]\n",
      "Epoch [299/300]\n",
      "Epoch [300/300]\n",
      "Loss: 9332.382080078125\n",
      "Loss: 3640.034912109375\n"
     ]
    }
   ],
   "source": [
    "net = NN(7,1).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    for batch_index, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "        predict = net(data)\n",
    "        loss = criterion(predict, targets)\n",
    "\n",
    "        # Backward pass: compute the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step: update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    loss_sum = 0\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            # Forward pass: compute the model output\n",
    "            y_pred = model(x.to(device))\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        loss_sum = loss_sum / len(test_loader)\n",
    "        print(f\"Loss: {loss_sum}\")\n",
    "    \n",
    "    model.train()  # Set the model back to training mode\n",
    "\n",
    "# Final accuracy check on training and test sets\n",
    "check_accuracy(train_loader, net)\n",
    "check_accuracy(test_loader, net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python binder_env",
   "language": "python",
   "name": "binder_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
