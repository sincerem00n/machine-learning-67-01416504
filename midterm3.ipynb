{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create neural network for training Variational autoencoder (VAE). (Use MNIST Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Dateset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(ConvolutionalVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten() # Flatten the output of the convolutional encoder\n",
    "        self.fc_mu = nn.Linear(32 * 7 * 7, latent_dim)  # Adjust the input size based on your image size and encoder layers\n",
    "        self.fc_logvar = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "        self.unflatten = nn.Unflatten(1, (32, 7, 7)) # Unflatten before the decoder\n",
    "        self.linear_decoder = nn.Linear(latent_dim, 32 * 7 * 7) \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        hidden = self.flatten(hidden) # Flatten for the fully connected layers\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        z = self.linear_decoder(z)\n",
    "        z = self.unflatten(z) # Unflatten before the convolutional decoder\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "\n",
    "def loss_function(reconstructed_x, x, mu, logvar):\n",
    "    reconstruction_loss = nn.MSELoss()(reconstructed_x, x)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + kl_divergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  10 Loss:  3.396404981613159\n",
      "iteration:  20 Loss:  2.2395801544189453\n",
      "iteration:  30 Loss:  1.9924795627593994\n",
      "iteration:  40 Loss:  1.8015836477279663\n",
      "iteration:  50 Loss:  1.6671171188354492\n",
      "iteration:  60 Loss:  1.5486398935317993\n",
      "iteration:  70 Loss:  1.4438987970352173\n",
      "iteration:  80 Loss:  1.3507699966430664\n",
      "iteration:  90 Loss:  1.25626540184021\n",
      "iteration:  100 Loss:  1.190504550933838\n",
      "iteration:  110 Loss:  1.1203495264053345\n",
      "iteration:  120 Loss:  1.0626089572906494\n",
      "iteration:  130 Loss:  1.0215226411819458\n",
      "iteration:  140 Loss:  1.0031187534332275\n",
      "iteration:  150 Loss:  0.9825464487075806\n",
      "iteration:  160 Loss:  0.9664984941482544\n",
      "iteration:  170 Loss:  0.9611880779266357\n",
      "iteration:  180 Loss:  0.9522223472595215\n",
      "iteration:  190 Loss:  0.9502744078636169\n",
      "iteration:  200 Loss:  0.9416422843933105\n",
      "iteration:  210 Loss:  0.9427673816680908\n",
      "iteration:  220 Loss:  0.9351805448532104\n",
      "iteration:  230 Loss:  0.9383175373077393\n",
      "iteration:  240 Loss:  0.9350405335426331\n",
      "iteration:  250 Loss:  0.9347307682037354\n",
      "iteration:  260 Loss:  0.9354968070983887\n",
      "iteration:  270 Loss:  0.9320793151855469\n",
      "iteration:  280 Loss:  0.9339562654495239\n",
      "iteration:  290 Loss:  0.9304879903793335\n",
      "iteration:  300 Loss:  0.9298449158668518\n",
      "iteration:  310 Loss:  0.9314775466918945\n",
      "iteration:  320 Loss:  0.9281553030014038\n",
      "iteration:  330 Loss:  0.931637704372406\n",
      "iteration:  340 Loss:  0.9291751980781555\n",
      "iteration:  350 Loss:  0.9290605783462524\n",
      "iteration:  360 Loss:  0.9277571439743042\n",
      "iteration:  370 Loss:  0.9308522939682007\n",
      "iteration:  380 Loss:  0.9281327128410339\n",
      "iteration:  390 Loss:  0.9268944263458252\n",
      "iteration:  400 Loss:  0.9261133670806885\n",
      "iteration:  410 Loss:  0.9295852184295654\n",
      "iteration:  420 Loss:  0.9284663796424866\n",
      "iteration:  430 Loss:  0.9300514459609985\n",
      "iteration:  440 Loss:  0.9260047078132629\n",
      "iteration:  450 Loss:  0.9237270951271057\n",
      "iteration:  460 Loss:  0.9266105890274048\n",
      "iteration:  470 Loss:  0.9236844778060913\n",
      "iteration:  480 Loss:  0.9262957572937012\n",
      "iteration:  490 Loss:  0.924854040145874\n",
      "iteration:  500 Loss:  0.9277805685997009\n",
      "iteration:  510 Loss:  0.9215147495269775\n",
      "iteration:  520 Loss:  0.925188422203064\n",
      "iteration:  530 Loss:  0.9237010478973389\n",
      "iteration:  540 Loss:  0.9228618741035461\n",
      "iteration:  550 Loss:  0.9256478548049927\n",
      "iteration:  560 Loss:  0.9278349876403809\n",
      "iteration:  570 Loss:  0.9263746738433838\n",
      "iteration:  580 Loss:  0.9270379543304443\n",
      "iteration:  590 Loss:  0.9275592565536499\n",
      "iteration:  600 Loss:  0.924963116645813\n",
      "iteration:  610 Loss:  0.9267616271972656\n",
      "iteration:  620 Loss:  0.927594006061554\n",
      "iteration:  630 Loss:  0.9264507293701172\n",
      "iteration:  640 Loss:  0.927854061126709\n",
      "iteration:  650 Loss:  0.9275193214416504\n",
      "iteration:  660 Loss:  0.9234400391578674\n",
      "iteration:  670 Loss:  0.9249398708343506\n",
      "iteration:  680 Loss:  0.9278552532196045\n",
      "iteration:  690 Loss:  0.9288619756698608\n",
      "iteration:  700 Loss:  0.9271841049194336\n",
      "iteration:  710 Loss:  0.9267370700836182\n",
      "iteration:  720 Loss:  0.9246394038200378\n",
      "iteration:  730 Loss:  0.9265726208686829\n",
      "iteration:  740 Loss:  0.924393892288208\n",
      "iteration:  750 Loss:  0.9271412491798401\n"
     ]
    }
   ],
   "source": [
    "vae_model = ConvolutionalVAE(latent_dim = 64)\n",
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr=0.001)\n",
    "\n",
    "epoch = 1\n",
    "for i in range(epoch):\n",
    "    ite = 0\n",
    "    for x,y in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, mu, logvar  = vae_model(x)\n",
    "        loss = loss_function(outputs, x, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ite += 1\n",
    "        \n",
    "        if ite % 10 == 0:\n",
    "            print(\"iteration: \", ite , \"Loss: \", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 100, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [-1.8285e-04, -4.5074e-05, -2.2319e-04,  ...,  5.8072e-05,\n",
       "           7.6843e-05,  2.1030e-04],\n",
       "         ...,\n",
       "         [-7.7269e-05, -8.1903e-05,  8.6583e-05,  ..., -1.8766e-04,\n",
       "           5.0944e-05, -7.6514e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [-4.9843e-04,  3.1446e-04, -8.0263e-05,  ..., -1.5430e-04,\n",
       "           3.2663e-04, -6.1707e-04]],\n",
       "\n",
       "        [[ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         ...,\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 1.4803e-05,  1.5644e-05,  1.8164e-05,  ..., -6.9057e-06,\n",
       "          -5.2655e-06,  7.4401e-06],\n",
       "         [-5.4527e-04, -1.6837e-04, -6.8394e-04,  ...,  1.8997e-04,\n",
       "           2.3058e-04,  5.8778e-04]],\n",
       "\n",
       "        [[ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         ...,\n",
       "         [ 6.9409e-05, -4.2525e-06,  8.4176e-05,  ...,  2.5037e-05,\n",
       "          -3.5726e-05,  4.0509e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 1.1989e-05,  6.2415e-05, -2.2824e-05,  ..., -4.9275e-05,\n",
       "          -1.2735e-05,  1.0324e-05]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         ...,\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05]],\n",
       "\n",
       "        [[ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [-8.4450e-04, -2.7016e-04, -1.0643e-03,  ...,  2.9888e-04,\n",
       "           3.5751e-04,  8.9945e-04],\n",
       "         [ 2.7105e-04, -4.8761e-04, -1.3006e-04,  ...,  6.4790e-04,\n",
       "          -1.1203e-03,  9.8707e-04],\n",
       "         ...,\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05]],\n",
       "\n",
       "        [[ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         ...,\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05],\n",
       "         [ 8.5118e-06,  2.0026e-05,  2.0080e-05,  ..., -1.1572e-05,\n",
       "          -4.3315e-06,  1.0980e-05]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model.eval\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu_list = []\n",
    "    logvar_list = []\n",
    "    for x,y in test_loader:\n",
    "        \n",
    "        _, mu, logvar = vae_model(x)\n",
    "        mu_list.append(mu)\n",
    "        logvar_list.append(logvar)\n",
    "\n",
    "logvar_list = torch.stack(logvar_list)     \n",
    "mu_list = torch.stack(mu_list)\n",
    "print(mu_list.shape)\n",
    "mu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
